{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama3 and open LLMs on AWS Trainium \n",
    "\n",
    "\n",
    "Open LLMs like Meta [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-70b), Mistral AI [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) & [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models or AI21 [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1) are now OpenAI competitors. However, most of the time you need to fine-tune the model on your data to unlock the full potential of the model. Fine-tuning smaller LLMs, like Mistral became very accessible but still require a lot of computational resources. Thats were AWS Trainium comes into play.\n",
    "\n",
    "\n",
    "This blog post walks you thorugh how to fine-tune a Llama 3.1 8B using [Hugging Face Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) on AWS Trainium. We will use the NeuronSFTTrainer to fine-tune the model on a custom dataset. The `NeuronSFTTrainer` is a high-level Trainer similar to the SFTTrainer from trl to easily supervise fine-tune LLMs on AWS Trainium with ease. Afterwards we will test the fine-tuned model with Hugging Face TGI. This post should provide you with a good starting point for fine-tuning and testing LLMs on AWS Accelerators, you will learn how to:\n",
    "\n",
    "1. [Setup AWS environment](#1-setup-aws-environment)\n",
    "2. [Create and prepare the dataset for fine-tuning](#2-create-and-prepare-the-dataset-for-fine-tuning)\n",
    "3. [Fine-tune Llama 3.1 using LoRA on AWS Trainium using the NeuronSFTTrainer](#3-fine-tune-llama-31-using-lora-on-aws-trainium-using-the-neurontrainer)\n",
    "4. [Run Inference with Hugging Face TGI](#4-run-inference-with-hugging-face-tgi)\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used. The accelerator supports a wide range of data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8. \n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | \\$1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | \\$21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | \\$24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.* \n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "This blog post doesn’t cover how to create the instance in detail. You can check out my previous blog about [“Setting up AWS Trainium for Hugging Face Transformers”](https://www.philschmid.de/setup-aws-trainium), which includes a step-by-step guide on setting up the environment. \n",
    "\n",
    "Once the instance is up and running, we can ssh into it. But instead of developing inside a terminal we want to use a `Jupyter` environment, which we can use for preparing our dataset and launching the training. For this, we need to add a port for forwarding in the `ssh` command, which will tunnel our localhost traffic to the Trainium instance.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\n",
    "KEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Next we need to clone the repository and change into the directory. Then we can launch the jupyter environment.\n",
    "\n",
    "```bash\n",
    "# clone repository\n",
    "git clone https://github.com/philschmid/llama3-aws-trainium-sample.git\n",
    "# change directory\n",
    "cd llama3-aws-trainium-sample/notebooks\n",
    "# launch jupyter\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "You should see a familiar **`jupyter`** output with a URL to the notebook.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "We can click on it, and a **`jupyter`** environment opens in our local browser. Open the notebook **`llama3-8b-fine-tuning.ipynb`** and lets get started.\n",
    "\n",
    "_Note: We are going to use the Jupyter environment only for preparing the dataset and then `torchrun` for launching our training script for distributed training._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start fine-tuning, we need to login into our Hugging Face account, which has access to the model, to use your token for accessing the gated repository. We can do this by running the following command:\n",
    "\n",
    "_Note: We also provide an ungated checkpoint for Llama 3.1 8B._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to make sure we have the latest `optimum-neuron` package and Hugging Face `trl` package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions used to test this notebook\n",
    "%pip install --upgrade \"optimum-neuron==0.0.26\" \"trl==0.11.4\" \"peft==0.13.2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset for fine-tuning\n",
    "\n",
    "After our environment is set up, we can start creating and preparing our dataset. A fine-tuning dataset should have a diverse set of demonstrations of the task you want to solve. If you want to learn more about how to create a dataset, take a look at the [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset).\n",
    "\n",
    "We will use the [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. This data can be used for supervised fine-tuning (SFT) to make language models follow instructions better. No Robots was modelled after the instruction dataset described in OpenAI's [InstructGPT paper](https://huggingface.co/papers/2203.02155), and is comprised mostly of single-turn instructions.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "The [no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset has 10,000 split into 9,500 training and 500 test examples. Some samples are not including a `system` message. We will load the dataset with the `datasets` library, add a missing `system` message and save them to separate json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "\n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 3.1 using LoRA on AWS Trainium using the NeuronSFTTrainer\n",
    "\n",
    "We are now ready to fine-tune our LLM with [NeuronSFTTrainer](https://huggingface.co/docs/optimum-neuron/package_reference/trainer) a 1-to-1 replacement for the Hugging Face `SFTTrainer` but for AWS Trainium instances.\n",
    "\n",
    "Every AWS Trainium instances comes with > 1 accelerators. This means we will always use distributed training. The `NeuronSFTTrainer` comes with different distributed training strategies including: \n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): shards the optimizer state over multiple devices.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html): shards the model parameters along a given dimension on multiple devices, defined with `tensor_parallel_size`\n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations on the sequence axis outside of the tensor parallel regions. It is useful because it saves memory by sharding the activations.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _coming soon_\n",
    "\n",
    "It also supports `SFTTrainer` features like:\n",
    "* Dataset formatting, including conversational and instruction format\n",
    "* Training on completions only, ignoring prompts\n",
    "* Packing datasets for more efficient training\n",
    "* PEFT (parameter-efficient fine-tuning) support including LoRA\n",
    "* Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "\n",
    "We prepared a script [run_sft.py](../scripts/run_sft.py) which will load the dataset from disk (json), prepare the model, tokenizer, create a LoRA configuration and making it straightfoward to supervise fine-tune open LLMs from YAML files. \n",
    "\n",
    "When training models on AWS Accelerators like AWS Trainium, we must first compile our model to enable execution on the specialized hardware. During compilation, the model's computational graph is optimized and translated into instructions specifically tailored for Trainium's NeuronCores, ensuring efficient utilization of the accelerator's capabilities.\n",
    "\n",
    "Model compilation is done using the `neuron_parallel_compile` with your training script, model and hyperparameters you plan to use during training, except that it only needs to be run on a few steps, e.g. `10`. The `run_sft.py` scripts support loading config files, which makes it easy to overwrite the default parameters.\n",
    "\n",
    "First lets create our config file `config.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3_8b.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Llama-3.1-8b\"    # Hugging Face model id\n",
    "dataset_path: \"train_dataset.json\"     # path to dataset\n",
    "max_seq_length: 1024                   # max sequence length for model and packing of the dataset\n",
    "packing: true                          # group multiple samples into one sample to accelerate training\n",
    "# training parameters\n",
    "output_dir: \"./llama3_trn\"             # output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 2.0e-4                  # learning rate for lora 10x higher\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 4         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "bf16: true                             # use bfloat16 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# distributed parameters\n",
    "tensor_parallel_size: 8                # number of tensor parallel groups\n",
    "zero_1: false                          # use zero stage 1, not needed for Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created our config file, we can pre-compile our model using the `neuron_parallel_compile` command. We will use the `run_sft.py` script to compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables for memory and precision\n",
    "export NEURON_FUSE_SOFTMAX=1\n",
    "export NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3\n",
    "export MALLOC_ARENA_MAX=64\n",
    "export NEURON_CC_FLAGS=\"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity --cache_dir=/home/ubuntu/cache_dir_neuron/\"\n",
    "export XLA_USE_BF16=1 \n",
    "export XLA_DOWNCAST_BF16=1\n",
    "\n",
    "# Run the parallel compilation\n",
    "neuron_parallel_compile torchrun \\\n",
    "    --nproc_per_node=32 \\\n",
    "    ../scripts/run_sft.py \\\n",
    "    --config llama_3_8b.yaml \\\n",
    "    --max_steps 10\n",
    "    \n",
    "# remove dummy artifacts which are created by the precompilation command\n",
    "rm -rf \"llama3_trn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Compiling without a cache can take ~20-40 minutes. It will also create dummy files in the `llama3_trn` during compilation you we have to remove them afterwards. We also need to add `MALLOC_ARENA_MAX=64` to limit the CPU allocation to avoid potential crashes, don't remove it for now._ \n",
    "\n",
    "After the compilation is done we can start our training with a similar command, we just need to remove the `neuron_parallel_compile` and `max_steps` and can launch the training with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set environment variables for memory and precision\n",
    "export NEURON_FUSE_SOFTMAX=1\n",
    "export NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3\n",
    "export MALLOC_ARENA_MAX=64\n",
    "export NEURON_CC_FLAGS=\"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity --cache_dir=/home/ubuntu/cache_dir_neuron/\"\n",
    "export XLA_USE_BF16=1 \n",
    "export XLA_DOWNCAST_BF16=1\n",
    "\n",
    "# Run the parallel compilation\n",
    "torchrun \\\n",
    "    --nproc_per_node=32 \\\n",
    "    ../scripts/run_sft.py \\\n",
    "    --config llama_3_8b.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training for 3 epochs on `no_robots` (9.5k samples) took 13 minutes. This leads to a cost of ~$4.6 for the e2e training on the trn1.32xlarge instance. Not Bad! \n",
    "\n",
    "But before we can share and test our model we need to consolidate our model. Since we used Tensor Parallelism during training, we need to consolidate the model weights before we can use it. Tensor Parallelism shards the model weights accross different workers, only sharded checkpoints will be saved during training.\n",
    "\n",
    "The Optimum CLI provides a way of doing that very easily via the `optimum neuron consolidate` command. Since we used LoRA we also need to merge the LoRA weights with the base model afterwards. We added a helper scripts [merge_adapter_weights.py](../scripts/merge_adapter_weights.py) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate tp adapter shards\n",
    "# !optimum-cli neuron consolidate llama3_trn/adapter_shards/ llama3_trn\n",
    "!optimum-cli neuron consolidate llama3_trn/shards/ llama3_trn\n",
    "# merge adapter weights\n",
    "!python ../scripts/merge_adapter_weights.py --peft_model_id llama3_trn --output_dir merged_llama3_8b\n",
    "# clear old repository\n",
    "# !rm -rf llama3_trn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference with Hugging Face TGI\n",
    "\n",
    "Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2 we need to compile our model for the correct use. We will use our Trainium instance for the inference test, but we recommend customer to switch to Inferentia2 for inference. \n",
    "\n",
    "Optimum Neuron implements similar to Transformers AutoModel classes for easy inference use. We will use  the `NeuronModelForCausalLM` class to load our vanilla transformers checkpoint and convert it to neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run --entrypoint optimum-cli \\\n",
    "  -v $(pwd)/llama3_trn:/model \\\n",
    "  -v $(pwd)/compiled_llama3_8b:/compiled_llama3_8b \\\n",
    "  --privileged \\\n",
    "  ghcr.io/huggingface/neuronx-tgi:latest \\\n",
    "  export neuron --model /model --batch_size 4 --sequence_length 4096 --auto_cast_type bf16 --num_cores 2 --task text-generation /compiled_llama3_8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optimum.neuron import NeuronModelForCausalLM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'bf16'}\n",
    "# input_shapes = {\"batch_size\": 4, \"sequence_length\": 4096}\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"llama3_trn\")\n",
    "# model = NeuronModelForCausalLM.from_pretrained(\n",
    "#         \"llama3_trn\",\n",
    "#         export=True,\n",
    "#         **compiler_args,\n",
    "#         **input_shapes)\n",
    "\n",
    "# # save the compiled model so we can load it with Hugging Face TGI \n",
    "# model.save_pretrained(\"compiled_llama3_8b\")\n",
    "# tokenizer.save_pretrained(\"compiled_llama3_8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Inference compilation can take ~25minutes. Luckily, you need to only run this onces. Since you can save the model afterwards. If you are going to run on Inferentia2 you need to recompile again. The compilation is parameter and hardware specific._\n",
    "\n",
    "After we compiled the model we deploy it to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are existing examples on how to deploy open LLMs on AWS Inferentia2 and AWS Trainium:\n",
    "* [Deploy Mixtral 8x7B on AWS Inferentia2 with Hugging Face Optimum](https://www.philschmid.de/inferentia2-mixtral-8x7b)\n",
    "* [Deploy Llama 3 70B on AWS Inferentia2 with Hugging Face Optimum](https://www.philschmid.de/inferentia2-llama3-70b)\n",
    "\n",
    "To test we we will use the neuron specific TGI docker container. \n",
    "\n",
    "_Note: We have to make sure to launch the container with the same parameters as we used during compilation._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker run -p 8080:80 -d --name tgi \\\n",
    "       -v $(pwd)/compiled_llama3_8b:/llama \\\n",
    "       --privileged \\\n",
    "       -e HF_TOKEN=${HF_TOKEN} \\\n",
    "       -e HF_AUTO_CAST_TYPE=\"bf16\" \\\n",
    "       -e HF_NUM_CORES=2 \\\n",
    "       ghcr.io/huggingface/neuronx-tgi:latest \\\n",
    "       --model-id llama \\\n",
    "       --max-batch-size 4 \\\n",
    "       --max-input-tokens 4000 \\\n",
    "       --max-total-tokens 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your container is running you can send requests using the `openai` or `huggingface_hub` python libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_base=\"http://localhost:8080/v1\",api_key=\"notNeeded\")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"compiled_llama3_8b\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    "\tstream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AWS stands for Amazon Web Services. AWS is a suite of remote computing services offered by Amazon. The most widely used of these include Amazon Elastic Compute Cloud (Amazon EC2), which provides resizable compute capacity in the cloud; Amazon Simple Storage Service (Amazon S3), which is an object storage service; and Amazon Elastic Block Store (Amazon EBS), which is designed to provide high performance, durable block storage volumes for use with AWS instances. AWS also provides other services, such as AWS Identity and Access Management (IAM), a service that enables organizations to control access to their AWS resources, and AWS Key Management Service (AWS KMS), which helps customers create and control the use of encryption keys.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, Don't forget to stop your container once you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop tgi && docker rm tgi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
